# A Survey of LLM-Based Methods for Synthetic Data Generation and the Rise of Agentic Workflows

## Abstract
The increasing reliance on high-quality datasets for artificial intelligence (AI) development highlights the need for synthetic data generation (SDG) to address data scarcity, privacy concerns, and acquisition costs. Large language models (LLMs) have emerged as key tools for SDG, enabling automated synthesis of diverse, high-quality data. Recent advancements introduce **agentic workflows**, where multiple LLM-powered agents collaborate to generate high-quality synthetic data. This survey systematically examines **architectural approaches** in LLM-based SDG, comparing **traditional single-LLM methods** with **agentic multi-agent workflows**. We analyze their differences in **efficiency, scalability, and data quality**, highlighting their strengths and limitations. Additionally, we identify key research gaps and propose future directions, including **workflow automation, open-source model integration, and improvements in data diversity**. By maintaining an up-to-date repository of research, tools, and datasets, this work serves as a resource for advancing SDG methodologies and optimizing AI-driven data synthesis.

## Contents
- ðŸ“„ **Paper**: The full research paper in PDF format.
- ðŸ“š **References**: A list of relevant studies and citations.
